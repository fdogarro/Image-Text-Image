





# !pip install easyocr torch transformers diffusers pycocotools torchvision






#import libraries

import easyocr
import cv2
from matplotlib import pyplot as plt
import numpy as np
from google import genai
from google.genai import types
from PIL import Image
from io import BytesIO
import base64
import requests
import os
import time
from PIL import Image as PILImage

import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from transformers import BlipProcessor, BlipForConditionalGeneration, BlipForQuestionAnswering
from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer
from transformers import AutoProcessor, AutoModelForCausalLM

from diffusers import StableDiffusionPipeline

from pycocotools.coco import COCO






def extract_text_from_image(image_path, languages=['en']):
  """Extracts text from an image using EasyOCR.

  Args:
    image_path: The path to the image file.
    languages: A list of language codes to use for OCR. Defaults to English ('en').

  Returns:
    A list of tuples, where each tuple contains the extracted text, bounding box coordinates, and confidence score.
  """
  reader = easyocr.Reader(languages)
  result = reader.readtext(image_path)
  return result


IMAGE_PATH = 'demo_image.png'
extracted_text = extract_text_from_image(IMAGE_PATH)
print(extracted_text)


extracted_text


def display_image_with_text(image_path, result):
  """Displays the image with detected text and bounding boxes.

  Args:
    image_path: The path to the image file.
    result: The output from extract_text_from_image function.
  """
  img = cv2.imread(image_path)
  for detection in result:
    top_left = tuple([int(val) for val in detection[0][0]])
    bottom_right = tuple([int(val) for val in detection[0][2]])
    text = detection[1]
    font = cv2.FONT_HERSHEY_SIMPLEX
    img = cv2.rectangle(img, top_left, bottom_right, (0, 255, 0), 5)
    img = cv2.putText(img, text, top_left, font, 1.5, (255, 255, 255), 3, cv2.LINE_AA)

  plt.imshow(img)
  plt.show()


display_image_with_text(IMAGE_PATH, extracted_text)


combined_text = ' '.join([detection[1] for detection in extracted_text])
print(combined_text)





def generate_and_display_images(api_key, caption, num_images=4):
  """Generates and displays images using Google's Imagen model.

  Args:
    api_key: Your Google Cloud API key.
    caption: The text prompt to use for image generation.
    num_images: The number of images to generate.
  """
  client = genai.Client(api_key=api_key)

  response = client.models.generate_images(
      model='imagen-3.0-generate-002',
      prompt=caption,
      config=types.GenerateImagesConfig(
          number_of_images=num_images,
      )
  )
  for generated_image in response.generated_images:
    image = Image.open(BytesIO(generated_image.image.image_bytes))
    image.show() #To see the output, run the code.
  return Image.open(BytesIO(response.generated_images[0].image.image_bytes))


def generate_and_display_image_stability_ai(api_key, combined_text, width=1024, height=1024, steps=40, cfg_scale=5, seed=0, samples=1):
  """Generates and displays an image using Stability AI's Stable Diffusion API.

  Args:
    api_key: Your Stability AI API key.
    combined_text: The text prompt to use for image generation.
    width: The width of the generated image.
    height: The height of the generated image.
    steps: The number of inference steps.
    cfg_scale: The classifier-free guidance scale.
    seed: The random seed.
    samples: The number of images to generate.
  """
  url = "https://api.stability.ai/v1/generation/stable-diffusion-xl-1024-v1-0/text-to-image"

  body = {
      "steps": steps,
      "width": width,
      "height": height,
      "seed": seed,
      "cfg_scale": cfg_scale,
      "samples": samples,
      "text_prompts": [
          {
              "text": combined_text,
              "weight": 1
          }
      ],
  }

  headers = {
      "Accept": "application/json",
      "Content-Type": "application/json",
      "Authorization": f"Bearer {api_key}", # Include API key in Authorization header
  }

  response = requests.post(url, headers=headers, json=body)

  if response.status_code != 200:
      raise Exception("Non-200 response: " + str(response.text))

  data = response.json()

  # Make sure the out directory exists
  if not os.path.exists("./out"):
      os.makedirs("./out")

  # Generate a unique filename using timestamp
  timestamp = int(time.time())
  filename = f"./out/txt2img_{timestamp}.png"

  for i, image in enumerate(data["artifacts"]):
      image_data = base64.b64decode(image["base64"])
      image = PILImage.open(BytesIO(image_data))
      image.save(filename)
      display(image) #To see the output, run the code.


generate_and_display_images('GOOGLE IMAGEN API KEY', combined_text, num_images=2)


generate_and_display_image_stability_ai(
    api_key="STABLE DIFFUSION API KEY",
    combined_text=combined_text
)





import torch
from transformers import BlipProcessor, BlipForConditionalGeneration, BlipForQuestionAnswering
from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer
from transformers import AutoProcessor, AutoModelForCausalLM





from PIL import Image
import requests
import torch
from transformers import BlipProcessor, BlipForConditionalGeneration

def generate_caption_blip(image_url_or_path):
    """Generates a caption for an image using the BLIP model.

    Args:
        image_url_or_path: The URL or local path to the image.

    Returns:
        The generated caption as a string.
    """
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
    model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)
    print(f"Using device: {device}")

    try:
        # Attempt to open as URL first
        raw_image = Image.open(requests.get(image_url_or_path, stream=True).raw).convert('RGB')
        image_source = image_url_or_path
    except:
        # If URL fails, try opening as local file
        raw_image = Image.open(image_url_or_path).convert('RGB')
        image_source = image_url_or_path

    print(f"Loaded image from: {image_source}")

    inputs = processor(raw_image, return_tensors="pt").to(device)

    print("Generating caption...")
    out = model.generate(**inputs)

    caption = processor.decode(out[0], skip_special_tokens=True)

    print("\nGenerated Description:")
    print(caption)

    return caption  # Return the caption


image_url = 'https://images.unsplash.com/photo-1511465390398-532913e8328d?q=80&w=2064&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D'
caption = generate_caption_blip(image_url)
print(caption)  # Print the returned caption


generate_and_display_images('IMAGEN API KEY', caption, num_images=2)


generate_and_display_image_stability_ai(
    api_key="STABLE DIFFUSION API KEY",
    combined_text=caption
)





from PIL import Image
import requests
from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer

def generate_caption_vit_gpt2(image_url_or_path):
    """Generates a caption for an image using the 'nlpconnect/vit-gpt2-image-captioning' model.

    Args:
        image_url_or_path: The URL or local path to the image.

    Returns:
        The generated caption as a string.
    """
    # --- Model Loading ---
    model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
    feature_extractor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
    tokenizer = AutoTokenizer.from_pretrained("nlpconnect/vit-gpt2-image-captioning")

    # Set model configuration for text generation
    model.config.decoder_start_token_id = tokenizer.cls_token_id
    model.config.pad_token_id = tokenizer.pad_token_id
    model.config.vocab_size = model.config.decoder.vocab_size
    model.config.max_length = 16
    model.config.num_beams = 4
    model.config.early_stopping = True
    model.config.no_repeat_ngram_size = 2
    model.config.length_penalty = 2.0

    # --- Image Loading ---
    try:
        # Attempt to open as URL first
        image = Image.open(requests.get(image_url_or_path, stream=True).raw).convert("RGB")
        image_source = image_url_or_path
    except:
        # If URL fails, try opening as local file
        image = Image.open(image_url_or_path).convert("RGB")
        image_source = image_url_or_path

    print(f"Successfully loaded image from {image_source}")

    # --- Caption Generation ---
    pixel_values = feature_extractor(images=image, return_tensors="pt").pixel_values
    generated_ids = model.generate(pixel_values=pixel_values)
    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

    print("Generated Caption:", generated_text)
    return generated_text


image_url = "http://images.cocodataset.org/val2017/000000039769.jpg"
caption = generate_caption_vit_gpt2(image_url)
print(caption)


generate_and_display_images('IMAGEN API KEY', caption, num_images=2)


generate_and_display_image_stability_ai(
    api_key="STABLE DIFFUSION API KEY",
    combined_text=caption
)





def generate_caption_git(image_url_or_path):
    """Generates a caption for an image using the 'microsoft/git-large-coco' model.

    Args:
        image_url_or_path: The URL or local path to the image.

    Returns:
        The generated caption as a string.
    """
    # Load GIT model and processor
    processor = AutoProcessor.from_pretrained("microsoft/git-large-coco")
    model = AutoModelForCausalLM.from_pretrained("microsoft/git-large-coco")

    device = "cuda" if torch.cuda.is_available() else "cpu"  # Determine device
    model.to(device)

    # Load the image
    try:
        # Attempt to open as URL first
        image = Image.open(requests.get(image_url_or_path, stream=True).raw).convert("RGB")
    except:
        # If URL fails, try opening as local file
        image = Image.open(image_url_or_path).convert("RGB")

    # Remove the line causing the error:
    # image = image.to(device) #Move image to device

    # Instead, move the pixel_values tensor to the device after processing:
    pixel_values = processor(images=image, return_tensors="pt").pixel_values.to(device)


    # Generate caption
    generated_ids = model.generate(
        pixel_values=pixel_values,
        max_length=50,
        num_beams=4
    )

    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True, legacy=False)[0]
    print("GIT Caption:", generated_text)
    return generated_text


image_url = "http://images.cocodataset.org/val2017/000000039769.jpg"  # Or local file path
caption = generate_caption_git(image_url)
print(caption)

# Now you can use 'caption' with your image generation functions


generate_and_display_images('IMAGEN API KEY', caption, num_images=2)


generate_and_display_image_stability_ai(
    api_key="STABLE DIFFUSION API KEY",
    combined_text=caption
)





# Load model and processor
processor = BlipProcessor.from_pretrained("Salesforce/blip-vqa-base")
model = BlipForQuestionAnswering.from_pretrained("Salesforce/blip-vqa-base")

# Load sample image
img_url = 'https://cdn.pixabay.com/photo/2025/05/04/17/47/dog-9578735_1280.jpg'
raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')

# Define question and process inputs
question = "What color is the woman's jacket?"
inputs = processor(raw_image, question, return_tensors="pt")

# Generate answer
out = model.generate(**inputs)
# Convert the tensor to a list of token IDs
decoded_ids = out.squeeze(0).tolist()  # Remove batch dimension and convert to list
# Decode the token IDs into text
answer = processor.decode(decoded_ids, skip_special_tokens=True)

print(f"Q: {question}")
print(f"A: {answer}")





# Download and setup COCO dataset
!mkdir -p /content/data/coco
!wget http://images.cocodataset.org/zips/val2017.zip -P /content/data/coco/
!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -P /content/data/coco/

# Unzip the files
!unzip /content/data/coco/val2017.zip -d /content/data/coco/
!unzip /content/data/coco/annotations_trainval2017.zip -d /content/data/coco/

# Clean up zip files to save space
!rm /content/data/coco/*.zip


# Configure paths for COCO dataset
COCO_DIR = '/content/data/coco/'
IMAGE_DIR = os.path.join(COCO_DIR, 'images')
ANNOTATION_FILE = os.path.join(COCO_DIR, 'annotations/captions_train2017.json')

# Initialize COCO API for caption annotations
try:
    coco = COCO(ANNOTATION_FILE)
    print('COCO dataset loaded successfully')
except Exception as e:
    print(f'Error loading COCO dataset: {e}')


class COCODataset(Dataset):
    def __init__(self, coco, image_dir, transform=None):
        self.coco = coco
        self.image_dir = image_dir
        self.transform = transform
        self.ids = list(self.coco.imgs.keys())

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, idx):
        img_id = self.ids[idx]
        img_info = self.coco.loadImgs(img_id)[0]
        image = Image.open(os.path.join(self.image_dir, img_info['file_name'])).convert('RGB')

        if self.transform:
            image = self.transform(image)

        # Get annotations (captions)
        ann_ids = self.coco.getAnnIds(imgIds=img_id)
        annotations = self.coco.loadAnns(ann_ids)
        captions = [ann['caption'] for ann in annotations]

        return image, captions


# Configure paths for COCO dataset
COCO_DIR = '/content/data/coco'
IMAGE_DIR = os.path.join(COCO_DIR, 'val2017')  # Using validation set as it's smaller
ANNOTATION_FILE = os.path.join(COCO_DIR, 'annotations/captions_val2017.json')

# Initialize COCO API for caption annotations
try:
    coco = COCO(ANNOTATION_FILE)
    print('COCO dataset loaded successfully')
except Exception as e:
    print(f'Error loading COCO dataset: {e}')

# Define image transformations
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# Create dataset instance
dataset = COCODataset(coco, IMAGE_DIR, transform=transform)

# Create data loader
data_loader = DataLoader(dataset, batch_size=1, shuffle=True)


# Load pre-trained image captioning model
def load_captioning_model():
    model_name = "nlpconnect/vit-gpt2-image-captioning"
    model = VisionEncoderDecoderModel.from_pretrained(model_name)
    feature_extractor = ViTImageProcessor.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    return model, feature_extractor, tokenizer

# Initialize image captioning models
print('Loading image captioning model...')
caption_model, feature_extractor, tokenizer = load_captioning_model()
print('Model loaded successfully')


# Load Stable Diffusion pipeline
def load_text2img_model():
    model_id = "stabilityai/stable-diffusion-2-1"
    pipe = StableDiffusionPipeline.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        use_safetensors=True
    )
    # Check if GPU is available, otherwise default to CPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    pipe.to(device)

    # Only enable offload strategies if on CUDA
    if device.type == "cuda":
        pipe.enable_attention_slicing()
        pipe.enable_model_cpu_offload()  # Offload to CPU if on a CUDA device
        pipe.enable_sequential_cpu_offload()

    return pipe

print('Loading text-to-image model...')
text2img_pipe = load_text2img_model()
print('Model loaded successfully')


def process_image_to_image(image, caption_model, feature_extractor, tokenizer, text2img_pipe):
    #Generate caption
    with torch.no_grad():  # Reduce memory usage
        # Move the image to the same device as the model
        device = caption_model.device  # Get the model's device
        image = image.to(device)

        # Convert the normalized tensor back to PIL Image for the feature extractor
        # First denormalize
        image_denorm = image * torch.tensor([0.229, 0.224, 0.225]).to(device)[:, None, None] + \
                      torch.tensor([0.485, 0.456, 0.406]).to(device)[:, None, None]
        # Clip values to be in the range [0, 1]
        image_denorm = torch.clamp(image_denorm, 0, 1)

        # Convert to PIL Image
        image_cpu = image_denorm.cpu()
        image_pil = transforms.ToPILImage()(image_cpu)

        print("Processing image with shape:", image.shape)

        # Process with feature extractor (it will handle the normalization internally)
        pixel_values = feature_extractor(images=image_pil, return_tensors="pt").pixel_values.to(device)
        print("Pixel values shape:", pixel_values.shape)
        print("Pixel values range:", pixel_values.min().item(), "to", pixel_values.max().item())

        generated_ids = caption_model.generate(
            pixel_values,
            max_length=30,
            num_beams=4,
            early_stopping=True,
            repetition_penalty=1.5,  # Penalize repetition more strongly
            length_penalty=1.0,
            num_return_sequences=1,
            bad_words_ids=[[tokenizer.encode("black and white", add_special_tokens=False)[0]]],  # Prevent "black and white" from appearing
        )
        generated_caption = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        print("Generated caption:", generated_caption)

    # 2. Generate new image from caption
    generated_image = text2img_pipe(
        generated_caption,
        num_inference_steps=20,     # Reduced from 30
        guidance_scale=7.0,
        height=512,                 # Reduced from default 768
        width=512,
        use_memory_efficient_attention=True
    ).images[0]

    return generated_caption, generated_image





# Test the pipeline for a single image
for images, _ in data_loader:
    # Process first image
    image = images[0]
    caption, new_image = process_image_to_image(
        image,
        caption_model,
        feature_extractor,
        tokenizer,
        text2img_pipe
    )

    # Display results
    plt.figure(figsize=(15, 5))

    plt.subplot(1, 3, 1)
    plt.imshow(image.permute(1, 2, 0))  # Permute to (H, W, C) for display
    plt.title('Original Image')
    plt.axis('off')

    plt.subplot(1, 3, 2)
    plt.text(0.5, 0.5, caption, ha='center', va='center', wrap=True)
    plt.title('Generated Caption')
    plt.axis('off')

    plt.subplot(1, 3, 3)
    plt.imshow(new_image)
    plt.title('Generated Image')
    plt.axis('off')

    plt.show()
    break  # Process only one image for testing


# Test the pipeline for 5 images
for i, (images, _) in enumerate(data_loader):
    if i >= 5:  # Stop after processing 5 images
        break

    # Process first image
    image = images[0]
    caption, new_image = process_image_to_image(
        image,
        caption_model,
        feature_extractor,
        tokenizer,
        text2img_pipe
    )

    # Display results
    plt.figure(figsize=(15, 5))

    plt.subplot(1, 3, 1)
    plt.imshow(image.permute(1, 2, 0))  # Permute to (H, W, C) for display
    plt.title('Original Image')
    plt.axis('off')

    plt.subplot(1, 3, 2)
    plt.text(0.5, 0.5, caption, ha='center', va='center', wrap=True)
    plt.title('Generated Caption')
    plt.axis('off')

    plt.subplot(1, 3, 3)
    plt.imshow(new_image)
    plt.title('Generated Image')
    plt.axis('off')

    plt.show()


import json
import os

# Replace with your notebook path
notebook_path = "Image_Text_Image_Architecture.ipynb"

# Read the notebook
with open(notebook_path, 'r', encoding='utf-8') as f:
    notebook_content = json.load(f)

# Fix the metadata structure
if 'metadata' not in notebook_content:
    notebook_content['metadata'] = {}
    
if 'widgets' not in notebook_content['metadata']:
    notebook_content['metadata']['widgets'] = {}
    
if 'state' not in notebook_content['metadata']['widgets']:
    notebook_content['metadata']['widgets']['state'] = {}

# Save the fixed notebook (create a backup first)
backup_path = notebook_path + '.backup'
os.rename(notebook_path, backup_path)

with open(notebook_path, 'w', encoding='utf-8') as f:
    json.dump(notebook_content, f, indent=2)

print(f"Notebook fixed! Original backed up to {backup_path}")







